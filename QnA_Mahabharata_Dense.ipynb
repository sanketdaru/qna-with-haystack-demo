{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Question Answering System backed by a Dense Retriever\n",
    "\n",
    "Dense Retrievers use neural network models to create “dense” embedding vectors. Within this family, there are two different approaches:\n",
    "  \n",
    "  a) Single encoder: Use a single model to embed both the query and the passage.  \n",
    "  b) Dual-encoder: Use two models, one to embed the query and one to embed the passage.  \n",
    "\n",
    "**Examples:** REALM, DPR, Sentence-Transformers\n",
    "\n",
    "**Pros:** Captures semantic similarity instead of “word matches” (for example, synonyms, related topics).\n",
    "\n",
    "**Cons:** Computationally more heavy to use, initial training of the model (though this is less of an issue nowadays as many pre-trained models are available and most of the time, it’s not needed to train the model)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on Embedding Retrieval\n",
    "\n",
    "We are going to use an EmbeddingRetriever with Sentence Transformers models.\n",
    "\n",
    "These models are trained to embed similar sentences close to each other in a shared embedding space.\n",
    "\n",
    "Some models have been fine-tuned on massive Information Retrieval data and can be used to retrieve documents based on a short query (for example, multi-qa-mpnet-base-dot-v1). There are others that are more suited to semantic similarity tasks where you are trying to find the most similar documents to a given document (for example, all-mpnet-base-v2). There are even models that are multilingual (for example, paraphrase-multilingual-mpnet-base-v2). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`  \n",
    "**NOTE** Skip if already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install greenlet\n",
    "GRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install farm-haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the logging level to INFO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if FAISS package is available\n",
    "\n",
    "As we are going to use FAISS (Facebook AI Similarity Search), we need to ensure its packages are available.  \n",
    "Read more about [FAISS] https://ai.facebook.com/tools/faiss/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip list | grep faiss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell will give an error if FAISS is not installed. If so, install it by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install faiss-cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if SQLAlchemy is the correct version\n",
    "\n",
    "There is an open issue w.r.t. an incorrect version of SQLAlchemy that gets pulled-in as part of a dependency resolution. Please check for presence of version `1.4.47` else force reinstall it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip list | grep SQLAlchemy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell will list the version of currently installed SQLAlchemy package. If it's not 1.4.47, force reinstall it by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --force-reinstall -v \"SQLAlchemy==1.4.47\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the DocumentStore\n",
    "\n",
    "We'll start creating our question answering system by initializing a [DocumentStore](https://docs.haystack.deepset.ai/docs/document_store).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new document store\n",
    "\n",
    "If you have never run this notebook earlier, create a new document store. \n",
    "\n",
    "**NOTE:** If you already have a document store and FAISS index created, skip this and intermediate steps and jump directly to section _Re-run with an existing document store_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "\n",
    "sql_url=\"sqlite:///mahabharata.db\"\n",
    "\n",
    "sqlite_document_store = FAISSDocumentStore(sql_url = sql_url, faiss_index_factory_str=\"Flat\", return_embedding=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Documents\n",
    "\n",
    "1. Download all the 18 parvas of Mahabharata from https://www.kaggle.com/datasets/tilakd/mahabharata Unzip it and place the .txt file in folder named `data/Mahabharata` under the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"data/Mahabharata\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert the raw text file to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_docs\n",
    "\n",
    "files_to_docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use a PreProcessor to split the raw doucments to multiple documents with given configuration.\n",
    "\n",
    "    *Note from Haystack:* Dense Retrievers are limited in the length of text that they can read in one pass. As such, it is important that Documents are not longer than the dense Retriever's maximum input length. By default, Haystack's DensePassageRetriever model has a maximum length of 256 tokens. As such, we recommend that Documents contain significantly less words. We have found decent performance with Documents around 100 words long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "# Use PreProcessor to create the document boundaries\n",
    "processor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap=0\n",
    ")\n",
    "\n",
    "docs = processor.process(files_to_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Save the processed documents to document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_document_store.write_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Initialize a EmbeddingRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=sqlite_document_store,\n",
    "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Update the embeddings to iterate over previously indexed documents and update their embedding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important:\n",
    "# Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
    "# previously indexed documents and update their embedding representation.\n",
    "# While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
    "# At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
    "sqlite_document_store.update_embeddings(retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Save the updated FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the FAISS index\n",
    "sqlite_document_store.save(\"mahabharata_faiss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run with an existing document store\n",
    "\n",
    "If you ran the notebook earlier and already have the SQLite database with corresponding embeddings available, load it and initialize the EmbeddingRetriever. If you just created the embeddings, skip this step as the document store and EmbeddingRetriever are already loaded and ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "# If FAISS is generated then load FAISS document store\n",
    "sqlite_document_store = FAISSDocumentStore.load(index_path=\"mahabharata_faiss\", config_path=\"mahabharata_faiss.json\")\n",
    "assert sqlite_document_store.faiss_index_factory_str == \"Flat\"\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=sqlite_document_store,\n",
    "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Reader\n",
    "\n",
    "Here we use a FARMReader with the deepset/roberta-base-squad2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "\n",
    "# Load a  local model or any of the QA models on\n",
    "# Hugging Face's model hub (https://huggingface.co/models)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Retriever-Reader Pipeline\n",
    "\n",
    "In this tutorial, we're using a ready-made pipeline called `ExtractiveQAPipeline`. It connects the Reader and the Retriever. The combination of the two speeds up processing because the Reader only processes the Documents that the Retriever has passed on. To learn more about pipelines, see [Pipelines](https://docs.haystack.deepset.ai/docs/pipelines).\n",
    "\n",
    "To create the pipeline, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can configure how many candidates the reader and retriever shall return\n",
    "# The higher top_k for retriever, the better (but also the slower) your answers.\n",
    "prediction = pipe.run(\n",
    "    query=\"Who is Krishna?\", \n",
    "    #query=\"Who is called the divine son of Devaki?\", \n",
    "    #query=\"Who received Lord Shiva's boon?\",\n",
    "    #query=\"Who desired to perform the Rajasuya sacrifice?\",\n",
    "    #query=\"Who asked Ekalavya for his thumb?\",\n",
    "    #query=\"Why did Drona award the brahmastra weapon to Arjuna?\",\n",
    "    #query=\"Who learned about bhargav astra?\",\n",
    "    #query=\"Who is the daughter of the sage Gautama?\",\n",
    "    params={\"Retriever\": {\"top_k\": 10}, \n",
    "            \"Reader\": {\"top_k\": 2}}\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"medium\" ## Choose from `minimum`, `medium`, and `all`\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('haystack_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "85ea2c107d7945555de8e73270cf8a4d668bafec7aac344fa62e3415dc7bf5ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
